{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvNiqrBYHw+DX+2zTEWeam"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIvFqLWckFWp","executionInfo":{"status":"ok","timestamp":1708789147749,"user_tz":-180,"elapsed":1663,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"outputId":"e333f823-ddb6-4357-ed9b-332a45a459b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!rm -rf Sign-Language-Digits-Dataset-master\n","!unzip drive/MyDrive/Sign-Language-Digits-Dataset-master.zip"],"metadata":{"id":"y0qxEz4OuKTX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import os\n","\n","NUM_EXAMPLES = 5\n","IMAGES_PATH = \"Sign-Language-Digits-Dataset-master/Dataset\"\n","\n","# Get the list of labels from the list of folder names.\n","labels = []\n","for i in os.listdir(IMAGES_PATH):\n","  if os.path.isdir(os.path.join(IMAGES_PATH, i)):\n","    labels.append(i)\n","\n","# Show the images.\n","for label in labels:\n","  label_dir = os.path.join(IMAGES_PATH, label)\n","  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n","  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n","  for i in range(NUM_EXAMPLES):\n","    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n","    axs[i].get_xaxis().set_visible(False)\n","    axs[i].get_yaxis().set_visible(False)\n","  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n","\n","plt.show()"],"metadata":{"id":"AfR-S8a0l3hi","executionInfo":{"status":"ok","timestamp":1708789219258,"user_tz":-180,"elapsed":1946,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d746448-f3ef-4c26-9148-23283484a2da"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-30-7c758032ac92>:17: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n","  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n"]}]},{"cell_type":"code","source":["!pip install -q mediapipe-model-maker"],"metadata":{"id":"PlFcscDvp3lJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mediapipe_model_maker import gesture_recognizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SF5Jr8Qfp86p","executionInfo":{"status":"ok","timestamp":1708787254815,"user_tz":-180,"elapsed":13200,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"outputId":"a1e851aa-07e9-443e-864e-6e299f504062"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["data = gesture_recognizer.Dataset.from_folder(\n","    dirname=IMAGES_PATH,\n","    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",")\n","\n","# Split the archive into training, validation and test dataset.\n","train_data, rest_data = data.split(0.8)\n","validation_data, test_data = rest_data.split(0.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ePqcYvFBqBkO","executionInfo":{"status":"ok","timestamp":1708789353998,"user_tz":-180,"elapsed":125483,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"outputId":"53801540-5405-4820-dad3-5ac7a07a0de6"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://storage.googleapis.com/mediapipe-assets/palm_detection_full.tflite to /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n","Downloading https://storage.googleapis.com/mediapipe-assets/hand_landmark_full.tflite to /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n","Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tar.gz to /tmp/model_maker/gesture_recognizer/gesture_embedder\n"]}]},{"cell_type":"code","source":["hparams = gesture_recognizer.HParams(export_dir=\"numbers_model\")\n","options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n","model = gesture_recognizer.GestureRecognizer.create(\n","    train_data=train_data,\n","    validation_data=validation_data,\n","    options=options\n",")"],"metadata":{"id":"khbuzUC2se2F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708789436302,"user_tz":-180,"elapsed":46740,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"outputId":"a3052412-e48f-411e-a8b3-4a6fb0c58d39"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," hand_embedding (InputLayer  [(None, 128)]             0         \n"," )                                                               \n","                                                                 \n"," batch_normalization (Batch  (None, 128)               512       \n"," Normalization)                                                  \n","                                                                 \n"," re_lu (ReLU)                (None, 128)               0         \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," custom_gesture_recognizer_  (None, 11)                1419      \n"," out (Dense)                                                     \n","                                                                 \n","=================================================================\n","Total params: 1931 (7.54 KB)\n","Trainable params: 1675 (6.54 KB)\n","Non-trainable params: 256 (1.00 KB)\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","678/678 [==============================] - 5s 6ms/step - loss: 1.2457 - categorical_accuracy: 0.4926 - val_loss: 0.2486 - val_categorical_accuracy: 0.8580 - lr: 0.0010\n","Epoch 2/10\n","678/678 [==============================] - 5s 7ms/step - loss: 0.7118 - categorical_accuracy: 0.7176 - val_loss: 0.1387 - val_categorical_accuracy: 0.9053 - lr: 9.9000e-04\n","Epoch 3/10\n","678/678 [==============================] - 3s 4ms/step - loss: 0.6071 - categorical_accuracy: 0.7677 - val_loss: 0.0996 - val_categorical_accuracy: 0.9586 - lr: 9.8010e-04\n","Epoch 4/10\n","678/678 [==============================] - 3s 4ms/step - loss: 0.5681 - categorical_accuracy: 0.7913 - val_loss: 0.0856 - val_categorical_accuracy: 0.9586 - lr: 9.7030e-04\n","Epoch 5/10\n","678/678 [==============================] - 4s 5ms/step - loss: 0.5367 - categorical_accuracy: 0.8060 - val_loss: 0.0756 - val_categorical_accuracy: 0.9586 - lr: 9.6060e-04\n","Epoch 6/10\n","678/678 [==============================] - 4s 5ms/step - loss: 0.5260 - categorical_accuracy: 0.8164 - val_loss: 0.0701 - val_categorical_accuracy: 0.9704 - lr: 9.5099e-04\n","Epoch 7/10\n","678/678 [==============================] - 3s 4ms/step - loss: 0.5105 - categorical_accuracy: 0.8186 - val_loss: 0.0655 - val_categorical_accuracy: 0.9645 - lr: 9.4148e-04\n","Epoch 8/10\n","678/678 [==============================] - 5s 7ms/step - loss: 0.4841 - categorical_accuracy: 0.8282 - val_loss: 0.0630 - val_categorical_accuracy: 0.9586 - lr: 9.3207e-04\n","Epoch 9/10\n","678/678 [==============================] - 3s 5ms/step - loss: 0.4890 - categorical_accuracy: 0.8326 - val_loss: 0.0539 - val_categorical_accuracy: 0.9586 - lr: 9.2274e-04\n","Epoch 10/10\n","678/678 [==============================] - 3s 4ms/step - loss: 0.4796 - categorical_accuracy: 0.8385 - val_loss: 0.0520 - val_categorical_accuracy: 0.9645 - lr: 9.1352e-04\n"]}]},{"cell_type":"code","source":["loss, acc = model.evaluate(test_data, batch_size=1)\n","print(f\"Test loss:{loss}, Test accuracy:{acc}\")"],"metadata":{"id":"hXrANni_sXP0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708789442491,"user_tz":-180,"elapsed":1792,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"outputId":"bfa03fa6-19ae-4fd5-9be4-47216281112f"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["170/170 [==============================] - 1s 2ms/step - loss: 0.0980 - categorical_accuracy: 0.9706\n","Test loss:0.09797937422990799, Test accuracy:0.970588207244873\n"]}]},{"cell_type":"code","source":["# Export the model bundle.\n","model.export_model()\n","\n","# Rename the file to be more descriptive.\n","!mv numbers_model/gesture_recognizer.task numbers.task"],"metadata":{"id":"096MTjyRsYP3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708789740754,"user_tz":-180,"elapsed":2255,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"outputId":"552af8bf-3357-4b29-a363-0182ea8fa487"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Using existing files at /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n","Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n","Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n","Using existing files at /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"numbers.task\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"B_8TJ4_-y-R4","executionInfo":{"status":"ok","timestamp":1708789767751,"user_tz":-180,"elapsed":319,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"outputId":"ff18dd15-ebb5-4079-ca97-f2afcabfa1ea"},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_227270b4-e60e-4ea2-ac64-0a50a3fb15f4\", \"numbers.task\", 8464471)"]},"metadata":{}}]},{"cell_type":"code","source":["# Imports neccessary modules.\n","import mediapipe as mp\n","from mediapipe.tasks import python\n","from mediapipe.tasks.python import vision\n","\n","# Create a GestureRecognizer object.\n","model_path = os.path.abspath(\"numbers.task\")\n","recognizer = vision.GestureRecognizer.create_from_model_path(model_path)\n","\n","# Load the input image.\n","image = mp.Image.create_from_file('2gesture.jpeg')\n","\n","# Run gesture recognition.\n","recognition_result = recognizer.recognize(image)\n","\n","# Display the most likely gesture.\n","top_gesture = recognition_result.gestures[0][0]\n","print(f\"Gesture recognized: {top_gesture.category_name} ({top_gesture.score})\")"],"metadata":{"id":"zWUipgYbsqwF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708789819406,"user_tz":-180,"elapsed":266,"user":{"displayName":"Faruk Ućgun","userId":"13100883422378302540"}},"outputId":"ce2450f8-6e0d-4475-ba3b-b6fab48de023"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Gesture recognized: 2 (0.9465010762214661)\n"]}]}]}